
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="https://kashishchanana.github.io/theme/pygments/friendly.min.css">



  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/solid.css">












 

<meta name="author" content="Kashish Chanana" />
<meta name="description" content="This blog post is a Part II of building an LLM from scratch. This is yet again inspired by a coding workshop - Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka. As in the below diagram of contructing an LLM ground up, let&#39;s assume that we&#39;ve …" />
<meta name="keywords" content="LLMs From Scratch, Architecture">


  <meta property="og:site_name" content="Hitchhiker's Guide To AI"/>
  <meta property="og:title" content="Coding an LLM Architecture"/>
  <meta property="og:description" content="This blog post is a Part II of building an LLM from scratch. This is yet again inspired by a coding workshop - Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka. As in the below diagram of contructing an LLM ground up, let&#39;s assume that we&#39;ve …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://kashishchanana.github.io/drafts/coding-an-llm-architecture.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2024-09-01 00:00:00-07:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="https://kashishchanana.github.io/author/kashish-chanana.html">
  <meta property="article:section" content="LLMs"/>
  <meta property="article:tag" content="LLMs From Scratch"/>
  <meta property="article:tag" content="Architecture"/>
  <meta property="og:image" content="images/test/ai.png">

  <title>Hitchhiker's Guide To AI &ndash; Coding an LLM Architecture</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="https://kashishchanana.github.io/">
      <img src="images/test/ai.png" alt="" title="">
    </a>

    <h1>
      <a href="https://kashishchanana.github.io/"></a>
    </h1>

    <p>Honestly started as a note-taking exercise!</p>



    <ul class="social">
      <li>
        <a class="sc-github"
           href="https://github.com/KashishChanana"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-linkedin"
           href="https://www.linkedin.com/in/kashishchanana/"
           target="_blank">
          <i class="fa-brands fa-linkedin"></i>
        </a>
      </li>
      <li>
        <a class="sc-twitter"
           href="https://twitter.com/chankashish"
           target="_blank">
          <i class="fa-brands fa-twitter"></i>
        </a>
      </li>
      <li>
        <a class="sc-envelope"
rel="me"           href="mailto:chananakashish1998@gmail.com"
           target="_blank">
          <i class="fa-solid fa-envelope"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>

<nav>
  <a href="https://kashishchanana.github.io/">Home</a>

  <a href="/archives">Archives</a>
  <a href="/categories">Categories</a>
  <a href="/tags">Tags</a>


</nav>

<article class="single">
  <header>
      
    <h1 id="coding-an-llm-architecture">Coding an LLM Architecture</h1>
    <p>
      Posted on Sun 01 September 2024 in <a href="https://kashishchanana.github.io/category/llms.html">LLMs</a>

    </p>
  </header>


  <div>
    <p>This blog post is a Part II of building an LLM from scratch. This is yet again inspired by a coding workshop - Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka. </p>
<p>As in the below diagram of contructing an LLM ground up, let's assume that we've a black box 'Attention Mechanism' ready with us, and we are jumping straight ahead to Part 3, LLM Architecture. I'd revisit Part 2 Attention Mechanism in a different blog post. This way of learning helps me, as I get to understand the high level architecture before diving deep into any of the phases! We will begin with a top-down view of the model architecture in the next section before covering the individual components in more detail.</p>
<p><img alt="LLM-Pipeline" src="images/llm-architecture/pipeline.png"></p>
<div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#coding-a-gpt-like-large-language-model-llm-that-can-be-trained-to-generate-human-like-text">Coding a GPT-like large language model (LLM) that can be trained to generate human-like text</a></li>
<li><a href="#normalizing-layer-activations-to-stabilize-neural-network-training">Normalizing layer activations to stabilize neural network training</a></li>
<li><a href="#adding-shortcut-connections-in-deep-neural-networks-to-train-models-more-effectively">Adding shortcut connections in deep neural networks to train models more effectively</a></li>
<li><a href="#implementing-transformer-blocks-to-create-gpt-models-of-various-sizes">Implementing transformer blocks to create GPT models of various sizes</a></li>
<li><a href="#computing-the-number-of-parameters-and-storage-requirements-of-gpt-models">Computing the number of parameters and storage requirements of GPT models</a></li>
</ul>
</div>
<h3 id="coding-a-gpt-like-large-language-model-llm-that-can-be-trained-to-generate-human-like-text">Coding a GPT-like large language model (LLM) that can be trained to generate human-like text</h3>
<p>LLMs, such as GPT (which stands for Generative Pretrained Transformer), are large deep
neural network architectures designed to generate new text one word (or token) at a time.</p>
<p><img alt="LLM-Pipeline" src="images/llm-architecture/raw-llm-archi.png"></p>
<p>The configuration of the small GPT-2 model via the following Python dictionary,
which we will use in the code examples later.</p>
<div class="highlight"><pre><span></span><code><span class="n">GPT_CONFIG_124M</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">50257</span><span class="p">,</span>    <span class="c1"># Vocabulary size</span>
    <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span> <span class="c1"># Context length</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>         <span class="c1"># Embedding dimension</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>          <span class="c1"># Number of attention heads</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>         <span class="c1"># Number of layers</span>
    <span class="s2">&quot;drop_rate&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>       <span class="c1"># Dropout rate</span>
    <span class="s2">&quot;qkv_bias&quot;</span><span class="p">:</span> <span class="kc">False</span>       <span class="c1"># Query-Key-Value bias</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="normalizing-layer-activations-to-stabilize-neural-network-training">Normalizing layer activations to stabilize neural network training</h3>
<h3 id="adding-shortcut-connections-in-deep-neural-networks-to-train-models-more-effectively">Adding shortcut connections in deep neural networks to train models more effectively</h3>
<h3 id="implementing-transformer-blocks-to-create-gpt-models-of-various-sizes">Implementing transformer blocks to create GPT models of various sizes</h3>
<h3 id="computing-the-number-of-parameters-and-storage-requirements-of-gpt-models">Computing the number of parameters and storage requirements of GPT models</h3>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://kashishchanana.github.io/tag/llms-from-scratch.html">LLMs From Scratch</a>
      <a href="https://kashishchanana.github.io/tag/architecture.html">Architecture</a>
    </p>
  </div>






</article>

<footer>
<p>&copy; 2024 Kashish Chanana</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Hitchhiker's Guide To AI ",
  "url" : "https://kashishchanana.github.io",
  "image": "images/test/ai.png",
  "description": ""
}
</script>
</body>
</html>