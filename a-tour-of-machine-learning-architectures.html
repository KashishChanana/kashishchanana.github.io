
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
  <link rel="icon" type="image/x-icon" href="/images/favicon.ico">

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="https://kashishchanana.github.io/theme/pygments/friendly.min.css">



  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/solid.css">












 

<meta name="author" content="Kashish Chanana" />
<meta name="description" content="Table of Contents Early Fusion Example (Feature-Level Fusion) Scenario Steps Example Code (Pseudocode) Advantages Limitations Late Fusion Example (Decision-Level Fusion) Scenario Steps Example Code (Pseudocode) Advantages Limitations When to Choose Early vs. Late Fusion Early Fusion Late Fusion Hybrid Fusion Multi-Label Classifier with Early Fusion Example Use Case Multi-Task Classifier …" />
<meta name="keywords" content="Architecture Patterns">


  <meta property="og:site_name" content="Hitchhiker's Guide To AI"/>
  <meta property="og:title" content="A Tour of Machine Learning Architectures"/>
  <meta property="og:description" content="Table of Contents Early Fusion Example (Feature-Level Fusion) Scenario Steps Example Code (Pseudocode) Advantages Limitations Late Fusion Example (Decision-Level Fusion) Scenario Steps Example Code (Pseudocode) Advantages Limitations When to Choose Early vs. Late Fusion Early Fusion Late Fusion Hybrid Fusion Multi-Label Classifier with Early Fusion Example Use Case Multi-Task Classifier …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://kashishchanana.github.io/a-tour-of-machine-learning-architectures.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2025-01-06 00:00:00-08:00"/>
  <meta property="article:modified_time" content="2025-01-06 00:00:00-08:00"/>
  <meta property="article:author" content="https://kashishchanana.github.io/author/kashish-chanana.html">
  <meta property="article:section" content="misc"/>
  <meta property="article:tag" content="Architecture Patterns"/>
  <meta property="og:image" content="images/test/ai.png">

  <title>Hitchhiker's Guide To AI &ndash; A Tour of Machine Learning Architectures</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="https://kashishchanana.github.io/">
      <img src="images/test/ai.png" alt="" title="">
    </a>

    <h1>
      <a href="https://kashishchanana.github.io/"></a>
    </h1>

    <p>Honestly started as a note-taking exercise!</p>



    <ul class="social">
      <li>
        <a class="sc-github"
           href="https://github.com/KashishChanana"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-linkedin"
           href="https://www.linkedin.com/in/kashishchanana/"
           target="_blank">
          <i class="fa-brands fa-linkedin"></i>
        </a>
      </li>
      <li>
        <a class="sc-twitter"
           href="https://twitter.com/chankashish"
           target="_blank">
          <i class="fa-brands fa-twitter"></i>
        </a>
      </li>
      <li>
        <a class="sc-envelope"
rel="me"           href="mailto:chananakashish1998@gmail.com"
           target="_blank">
          <i class="fa-solid fa-envelope"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>

<nav>
  <a href="https://kashishchanana.github.io/">Home</a>

  <a href="/archives">Archives</a>
  <a href="/categories">Categories</a>
  <a href="/tags">Tags</a>


</nav>

<article class="single">
  <header>
      
    <h1 id="a-tour-of-machine-learning-architectures">A Tour of Machine Learning Architectures</h1>
    <p>
      Posted on Mon 06 January 2025 in <a href="https://kashishchanana.github.io/category/misc.html">misc</a>

    </p>
  </header>


  <div>
    <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#early-fusion-example-feature-level-fusion">Early Fusion Example (Feature-Level Fusion)</a><ul>
<li><a href="#scenario">Scenario</a></li>
<li><a href="#steps">Steps</a></li>
<li><a href="#example-code-pseudocode">Example Code (Pseudocode)</a></li>
<li><a href="#advantages">Advantages</a></li>
<li><a href="#limitations">Limitations</a></li>
</ul>
</li>
<li><a href="#late-fusion-example-decision-level-fusion">Late Fusion Example (Decision-Level Fusion)</a><ul>
<li><a href="#scenario_1">Scenario</a></li>
<li><a href="#steps_1">Steps</a></li>
<li><a href="#example-code-pseudocode_1">Example Code (Pseudocode)</a></li>
<li><a href="#advantages_1">Advantages</a></li>
<li><a href="#limitations_1">Limitations</a></li>
</ul>
</li>
<li><a href="#when-to-choose-early-vs-late-fusion">When to Choose Early vs. Late Fusion</a><ul>
<li><a href="#early-fusion">Early Fusion</a></li>
<li><a href="#late-fusion">Late Fusion</a></li>
</ul>
</li>
<li><a href="#_1"></a><ul>
<li><a href="#hybrid-fusion">Hybrid Fusion</a></li>
</ul>
</li>
<li><a href="#multi-label-classifier-with-early-fusion">Multi-Label Classifier with Early Fusion</a><ul>
<li><a href="#example-use-case">Example Use Case</a></li>
</ul>
</li>
<li><a href="#multi-task-classifier">Multi-Task Classifier</a><ul>
<li><a href="#1-shared-feature-extractor">1. Shared Feature Extractor</a></li>
<li><a href="#2-task-specific-heads">2. Task-Specific Heads</a></li>
<li><a href="#3-loss-functions">3. Loss Functions</a></li>
<li><a href="#4-training">4. Training</a></li>
<li><a href="#model-architecture">Model Architecture</a></li>
<li><a href="#example-code-tensorflowkeras">Example Code (TensorFlow/Keras)</a></li>
<li><a href="#training-the-model">Training the Model</a></li>
<li><a href="#key-points-to-consider">Key Points to Consider</a></li>
</ul>
</li>
</ul>
</div>
<h3 id="early-fusion-example-feature-level-fusion">Early Fusion Example (Feature-Level Fusion)</h3>
<h4 id="scenario">Scenario</h4>
<p>You want to predict a user's sentiment based on textual data and a corresponding image(e.g., a post on social media).</p>
<h4 id="steps">Steps</h4>
<ol>
<li>Extract features from both modalities</li>
<li>Use a Text Encoder (e.g., a pre-trained BERT model) to generate embeddings for the text.</li>
<li>Use an Image Encoder (e.g., ResNet or a CNN) to extract features from the image.</li>
<li>Concatenate the feature vectors (early fusion)
   <code>Combined_Features = Concatenate([Text_Features, Image_Features])</code></li>
<li>Feed the combined features into a single machine learning model (e.g., a fully connected neural network).</li>
<li>Train the model to predict sentiment based on the unified representation.</li>
</ol>
<h4 id="example-code-pseudocode">Example Code (Pseudocode)</h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Extract text and image features</span>
<span class="n">text_features</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">text_input</span><span class="p">)</span>
<span class="n">image_features</span> <span class="o">=</span> <span class="n">resnet_model</span><span class="p">(</span><span class="n">image_input</span><span class="p">)</span>

<span class="c1"># Early fusion Combine features</span>
<span class="n">combined_features</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">text_features</span><span class="p">,</span> <span class="n">image_features</span><span class="p">])</span>

<span class="c1"># Feed into a unified model</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">dense_layer</span><span class="p">(</span><span class="n">combined_features</span><span class="p">)</span>
</code></pre></div>

<h4 id="advantages">Advantages</h4>
<ul>
<li>Captures interactions between text and image data.</li>
<li>Single model simplifies deployment.</li>
</ul>
<h4 id="limitations">Limitations</h4>
<ul>
<li>Requires careful handling of different feature scales.</li>
<li>Can lead to very large input sizes.</li>
</ul>
<hr>
<h3 id="late-fusion-example-decision-level-fusion">Late Fusion Example (Decision-Level Fusion)</h3>
<h4 id="scenario_1">Scenario</h4>
<p>You want to classify news articles using text data and reader engagement metrics (e.g., clicks, likes).</p>
<h4 id="steps_1">Steps</h4>
<ol>
<li>Train separate models for each modality</li>
<li>A Text Classification Model (e.g., using a transformer) to classify the text.</li>
<li>A Tabular Data Model (e.g., XGBoost) to predict based on engagement metrics.</li>
<li>Generate predictions (e.g., probabilities) from each model
   <code>Text_Model_Prediction = [0.7, 0.2, 0.1]  # Probabilities for 3 classes
   Tabular_Model_Prediction = [0.6, 0.3, 0.1]</code></li>
<li>Combine predictions using a late fusion technique</li>
<li>Weighted average
     <code>Final_Prediction = 0.6 * Text_Model_Prediction + 0.4 * Tabular_Model_Prediction</code></li>
<li>
<p>Voting or ensemble techniques.</p>
</li>
<li>
<p>Use the combined prediction for the final decision.</p>
</li>
</ol>
<h4 id="example-code-pseudocode_1">Example Code (Pseudocode)</h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Generate predictions from individual models</span>
<span class="n">text_pred</span> <span class="o">=</span> <span class="n">text_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">text_input</span><span class="p">)</span>
<span class="n">tabular_pred</span> <span class="o">=</span> <span class="n">tabular_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">tabular_input</span><span class="p">)</span>

<span class="c1"># Late fusion Combine predictions (weighted average)</span>
<span class="n">final_pred</span> <span class="o">=</span> <span class="mf">0.6</span> <span class="o">*</span> <span class="n">text_pred</span> <span class="o">+</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">tabular_pred</span>
</code></pre></div>

<h4 id="advantages_1">Advantages</h4>
<ul>
<li>Each model specializes in its own data modality.</li>
<li>More robust to noisy data in one source.</li>
</ul>
<h4 id="limitations_1">Limitations</h4>
<ul>
<li>Does not capture direct interactions between text and engagement features.</li>
<li>Coordination between models can increase complexity.</li>
</ul>
<hr>
<h3 id="when-to-choose-early-vs-late-fusion">When to Choose Early vs. Late Fusion</h3>
<h4 id="early-fusion">Early Fusion</h4>
<ul>
<li>If feature interactions are important, such as combining complementary information from different modalities (e.g., text and image for sentiment).</li>
<li>When the data sources have similar structures, enabling feature-level merging.</li>
</ul>
<h4 id="late-fusion">Late Fusion</h4>
<ul>
<li>If data sources are heterogeneous or require specialized models (e.g., image vs. tabular data).</li>
<li>When leveraging ensemble techniques to combine strengths of different models.</li>
<li>To add modularity for easier updates (e.g., replacing one model without retraining the entire system).</li>
</ul>
<h2 id="_1"><img alt="late-vs-early" src="images/machine-learning-archi-tour/late-vs-early.png"></h2>
<h3 id="hybrid-fusion">Hybrid Fusion</h3>
<p>You can also mix both techniques
- Combine some features early (e.g., combine image and audio features for a video analysis model).
- Use late fusion to merge the prediction with another modality (e.g., textual analysis of video captions).</p>
<h2 id="multi-label-classifier-with-early-fusion">Multi-Label Classifier with Early Fusion</h2>
<p>A multi-label classifier predicts multiple labels simultaneously (e.g., tagging an image with multiple attributes like "cat," "outdoor," "sunny"). Combining data from multiple sources (e.g., images and text) with early fusion involves merging features at the input level. Here's a step-by-step guide.</p>
<ol>
<li>Data Collection
Ensure you have multiple data sources for each instance, such as:
Images: Visual content of the instance.
Text: Descriptions, captions, or metadata.
Tabular Data (optional): Associated numerical or categorical features.</li>
<li>Preprocessing
Image Data:
Normalize pixel values (e.g., scale to [0, 1]).
Use an image encoder (e.g., ResNet, EfficientNet) to extract feature vectors.
Text Data:
Tokenize and process text (e.g., using a pre-trained model like BERT) to generate embeddings.
Tabular Data (if applicable):
Normalize numerical features and encode categorical features.</li>
<li>Feature Fusion (Early Fusion)
Extract features from each modality and concatenate them to create a unified representation.
Ensure all feature vectors have compatible dimensions (use projections if needed).</li>
<li>Multi-Label Classification Model
Use a fully connected neural network (dense layers) as the classifier.
The output layer should have one neuron per label, with an activation function such as sigmoid (to output probabilities for each label).</li>
<li>Loss Function
Use Binary Cross-Entropy Loss for multi-label classification:
plaintext
Copy code
Loss = - ∑ [y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred)]
This treats each label as an independent binary classification problem.</li>
<li>Training
Train the model using the combined feature representation as input and the multi-label target as the output.</li>
</ol>
<p>Image Input --&gt; Image Encoder --&gt; Image Features
Text Input  --&gt; Text Encoder  --&gt; Text Features
Tabular Data (Optional)       --&gt; Tabular Features</p>
<p>Combined Features = Concatenate([Image Features, Text Features, Tabular Features])</p>
<p>Combined Features --&gt; Dense Layers --&gt; Output Layer (Sigmoid Activations for Multi-Label Predictions)</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFBertModel</span>

<span class="c1"># Input layers for different data sources</span>
<span class="n">image_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;image_input&quot;</span><span class="p">)</span>
<span class="n">text_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">512</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;text_input&quot;</span><span class="p">)</span>  <span class="c1"># Assume 512 tokens</span>
<span class="n">tabular_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;tabular_input&quot;</span><span class="p">)</span>  <span class="c1"># Example tabular data with 10 features</span>

<span class="c1"># Image Encoder (e.g., ResNet)</span>
<span class="n">image_encoder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">ResNet50</span><span class="p">(</span><span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pooling</span><span class="o">=</span><span class="s2">&quot;avg&quot;</span><span class="p">)</span>
<span class="n">image_features</span> <span class="o">=</span> <span class="n">image_encoder</span><span class="p">(</span><span class="n">image_input</span><span class="p">)</span>

<span class="c1"># Text Encoder (e.g., BERT)</span>
<span class="n">bert_model</span> <span class="o">=</span> <span class="n">TFBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">text_features</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">text_input</span><span class="p">)</span><span class="o">.</span><span class="n">pooler_output</span>

<span class="c1"># Optional Tabular Features</span>
<span class="n">tabular_features</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">tabular_input</span><span class="p">)</span>

<span class="c1"># Early fusion: Combine features</span>
<span class="n">combined_features</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">image_features</span><span class="p">,</span> <span class="n">text_features</span><span class="p">,</span> <span class="n">tabular_features</span><span class="p">])</span>

<span class="c1"># Dense layers for classification</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">combined_features</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_labels</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;multi_label_output&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Sigmoid for multi-label</span>

<span class="c1"># Build and compile the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">image_input</span><span class="p">,</span> <span class="n">text_input</span><span class="p">,</span> <span class="n">tabular_input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
</code></pre></div>

<p>To create a multi-task classifier, where you handle multiple classification tasks simultaneously, you follow a similar approach to the general multi-task learning process but tailor it specifically for classification tasks.</p>
<hr>
<h3 id="example-use-case">Example Use Case</h3>
<p>Imagine a dataset containing textual data about customer reviews. You want to:
1. Task 1: Predict the review's sentiment (multi-class classification: positive, neutral, negative).
2. Task 2: Predict the review's category (multi-class classification: electronics, books, clothing).
3. Task 3: Identify relevant tags (multi-label classification: "cheap," "durable," "trendy").</p>
<hr>
<h2 id="multi-task-classifier">Multi-Task Classifier</h2>
<hr>
<h4 id="1-shared-feature-extractor">1. Shared Feature Extractor</h4>
<ul>
<li>Use a shared model (e.g., a BERT encoder or CNN) to extract features from the input data.</li>
<li>This layer learns a common representation useful across all tasks.</li>
</ul>
<h4 id="2-task-specific-heads">2. Task-Specific Heads</h4>
<ul>
<li>Add separate fully connected layers for each classification task:</li>
<li>Task 1: A dense layer with softmax activation for sentiment prediction.</li>
<li>Task 2: A dense layer with softmax activation for category prediction.</li>
<li>Task 3: A dense layer with sigmoid activation for multi-label classification.</li>
</ul>
<h4 id="3-loss-functions">3. Loss Functions</h4>
<ul>
<li>Each task uses an appropriate loss function:</li>
<li>Task 1 &amp; Task 2: Categorical Cross-Entropy.</li>
<li>
<p>Task 3: Binary Cross-Entropy.</p>
</li>
<li>
<p>Combine the losses into a weighted sum during training.</p>
</li>
</ul>
<h4 id="4-training">4. Training</h4>
<ul>
<li>Provide labels for all tasks in the training loop.</li>
<li>Train the model to optimize all tasks simultaneously.</li>
</ul>
<hr>
<h3 id="model-architecture">Model Architecture</h3>
<div class="highlight"><pre><span></span><code><span class="n">Input</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="n">text</span><span class="w"> </span><span class="kr">or</span><span class="w"> </span><span class="n">image</span><span class="p">)</span>
<span class="w">        </span><span class="o">|</span>
<span class="kr">Shared</span><span class="w"> </span><span class="n">Feature</span><span class="w"> </span><span class="n">Extractor</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="n">BERT</span><span class="p">,</span><span class="w"> </span><span class="n">ResNet</span><span class="p">,</span><span class="w"> </span><span class="n">CNN</span><span class="p">)</span>
<span class="w">        </span><span class="o">|</span>
<span class="w">        </span><span class="o">+--&gt;</span><span class="w"> </span><span class="n">Task</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Head</span><span class="w"> </span><span class="p">(</span><span class="n">Sentiment</span><span class="w"> </span><span class="n">Classification</span><span class="p">,</span><span class="w"> </span><span class="n">Softmax</span><span class="p">)</span>
<span class="w">        </span><span class="o">|</span>
<span class="w">        </span><span class="o">+--&gt;</span><span class="w"> </span><span class="n">Task</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">Head</span><span class="w"> </span><span class="p">(</span><span class="n">Category</span><span class="w"> </span><span class="n">Classification</span><span class="p">,</span><span class="w"> </span><span class="n">Softmax</span><span class="p">)</span>
<span class="w">        </span><span class="o">|</span>
<span class="w">        </span><span class="o">+--&gt;</span><span class="w"> </span><span class="n">Task</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">Head</span><span class="w"> </span><span class="p">(</span><span class="n">Tag</span><span class="w"> </span><span class="n">Classification</span><span class="p">,</span><span class="w"> </span><span class="n">Sigmoid</span><span class="p">)</span>
</code></pre></div>

<hr>
<h3 id="example-code-tensorflowkeras">Example Code (TensorFlow/Keras)</h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFBertModel</span>

<span class="c1"># Input Layer</span>
<span class="n">input_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">512</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;text_input&quot;</span><span class="p">)</span>  <span class="c1"># Example for text input (BERT token IDs)</span>

<span class="c1"># Shared Feature Extractor (e.g., BERT)</span>
<span class="n">bert_model</span> <span class="o">=</span> <span class="n">TFBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">shared_features</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">input_layer</span><span class="p">)</span><span class="o">.</span><span class="n">pooler_output</span>  <span class="c1"># Extract shared features</span>

<span class="c1"># Task 1: Sentiment Classification (Multi-class)</span>
<span class="n">task1_output</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;task1_output&quot;</span><span class="p">)(</span><span class="n">shared_features</span><span class="p">)</span>

<span class="c1"># Task 2: Category Classification (Multi-class)</span>
<span class="n">task2_output</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;task2_output&quot;</span><span class="p">)(</span><span class="n">shared_features</span><span class="p">)</span>

<span class="c1"># Task 3: Tag Classification (Multi-label)</span>
<span class="n">task3_output</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;task3_output&quot;</span><span class="p">)(</span><span class="n">shared_features</span><span class="p">)</span>

<span class="c1"># Create the Multi-Task Model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">task1_output</span><span class="p">,</span> <span class="n">task2_output</span><span class="p">,</span> <span class="n">task3_output</span><span class="p">])</span>

<span class="c1"># Compile the Model with Task-Specific Losses</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;task1_output&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical_crossentropy&quot;</span><span class="p">,</span>
        <span class="s2">&quot;task2_output&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical_crossentropy&quot;</span><span class="p">,</span>
        <span class="s2">&quot;task3_output&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;task1_output&quot;</span><span class="p">:</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
        <span class="s2">&quot;task2_output&quot;</span><span class="p">:</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
        <span class="s2">&quot;task3_output&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_accuracy&quot;</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># Display the Model Summary</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div>

<hr>
<h3 id="training-the-model">Training the Model</h3>
<p>Prepare the data:
- For Task 1 &amp; Task 2, labels should be one-hot encoded vectors.
- For Task 3, labels should be binary vectors indicating the presence/absence of each tag.</p>
<p>Train the model:</p>
<div class="highlight"><pre><span></span><code><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">text_data</span><span class="p">,</span>  <span class="c1"># Input data (e.g., tokenized text for BERT)</span>
    <span class="n">y</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;task1_output&quot;</span><span class="p">:</span> <span class="n">sentiment_labels</span><span class="p">,</span>  <span class="c1"># Labels for Task 1</span>
        <span class="s2">&quot;task2_output&quot;</span><span class="p">:</span> <span class="n">category_labels</span><span class="p">,</span>  <span class="c1"># Labels for Task 2</span>
        <span class="s2">&quot;task3_output&quot;</span><span class="p">:</span> <span class="n">tag_labels</span><span class="p">,</span>  <span class="c1"># Labels for Task 3</span>
    <span class="p">},</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
</code></pre></div>

<hr>
<h3 id="key-points-to-consider">Key Points to Consider</h3>
<ol>
<li>Loss Weighting:
Adjust the weights for each task's loss if one task is more important or has different scales:</li>
</ol>
<div class="highlight"><pre><span></span><code>   <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
       <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span>
       <span class="n">loss</span><span class="o">=</span><span class="p">{</span>
           <span class="s2">&quot;task1_output&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical_crossentropy&quot;</span><span class="p">,</span>
           <span class="s2">&quot;task2_output&quot;</span><span class="p">:</span> <span class="s2">&quot;categorical_crossentropy&quot;</span><span class="p">,</span>
           <span class="s2">&quot;task3_output&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span>
       <span class="p">},</span>
       <span class="n">loss_weights</span><span class="o">=</span><span class="p">{</span>
           <span class="s2">&quot;task1_output&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
           <span class="s2">&quot;task2_output&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
           <span class="s2">&quot;task3_output&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
       <span class="p">},</span>
       <span class="n">metrics</span><span class="o">=</span><span class="p">{</span>
           <span class="s2">&quot;task1_output&quot;</span><span class="p">:</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
           <span class="s2">&quot;task2_output&quot;</span><span class="p">:</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
           <span class="s2">&quot;task3_output&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_accuracy&quot;</span><span class="p">,</span>
       <span class="p">}</span>
   <span class="p">)</span>
</code></pre></div>

<ol>
<li>
<p>Evaluation:
   Use task-specific evaluation metrics (e.g., precision, recall, F1-score for multi-label tasks).</p>
</li>
<li>
<p>Shared Representation:
   The shared feature extractor should be powerful enough to learn representations useful for all tasks. If tasks are too different, consider partially shared layers or a hybrid approach.</p>
</li>
<li>
<p>Transfer Learning:
   You can initialize the shared extractor with pre-trained models (e.g., BERT for text, ResNet for images) to improve performance.</p>
</li>
</ol>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://kashishchanana.github.io/tag/architecture-patterns.html">Architecture Patterns</a>
    </p>
  </div>






</article>

<footer>
<p>&copy; 2024 Kashish Chanana</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Hitchhiker's Guide To AI ",
  "url" : "https://kashishchanana.github.io",
  "image": "images/test/ai.png",
  "description": ""
}
</script>
</body>
</html>