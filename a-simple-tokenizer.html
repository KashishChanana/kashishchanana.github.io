
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="https://kashishchanana.github.io/theme/pygments/friendly.min.css">



  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/solid.css">












 

<meta name="author" content="Kashish Chanana" />
<meta name="description" content="It&#39;s the Labor Day weekend, and I have found myself an amazing coding workshop by Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka, so here I am writing, understanding and building &#39;A Simple Tokenizer&#39; from scratch. The figures in this blog post are taken from …" />
<meta name="keywords" content="LLMs From Scratch, Tokenizer">


  <meta property="og:site_name" content="Hitchhiker's Guide To AI"/>
  <meta property="og:title" content="A Simple Tokenizer"/>
  <meta property="og:description" content="It&#39;s the Labor Day weekend, and I have found myself an amazing coding workshop by Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka, so here I am writing, understanding and building &#39;A Simple Tokenizer&#39; from scratch. The figures in this blog post are taken from …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://kashishchanana.github.io/a-simple-tokenizer.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2024-08-31 00:00:00-07:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="https://kashishchanana.github.io/author/kashish-chanana.html">
  <meta property="article:section" content="Scratch"/>
  <meta property="article:tag" content="LLMs From Scratch"/>
  <meta property="article:tag" content="Tokenizer"/>
  <meta property="og:image" content="images/test/ai.png">

  <title>Hitchhiker's Guide To AI &ndash; A Simple Tokenizer</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="https://kashishchanana.github.io/">
      <img src="images/test/ai.png" alt="" title="">
    </a>

    <h1>
      <a href="https://kashishchanana.github.io/"></a>
    </h1>

    <p>Honestly started as a note-taking exercise!</p>



    <ul class="social">
      <li>
        <a class="sc-github"
           href="https://github.com/KashishChanana"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-linkedin"
           href="https://www.linkedin.com/in/kashishchanana/"
           target="_blank">
          <i class="fa-brands fa-linkedin"></i>
        </a>
      </li>
      <li>
        <a class="sc-twitter"
           href="https://twitter.com/chankashish"
           target="_blank">
          <i class="fa-brands fa-twitter"></i>
        </a>
      </li>
      <li>
        <a class="sc-envelope"
rel="me"           href="mailto:chananakashish1998@gmail.com"
           target="_blank">
          <i class="fa-solid fa-envelope"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>

<nav>
  <a href="https://kashishchanana.github.io/">Home</a>

  <a href="/archives">Archives</a>
  <a href="/categories">Categories</a>
  <a href="/tags">Tags</a>


</nav>

<article class="single">
  <header>
      
    <h1 id="a-simple-tokenizer">A Simple Tokenizer</h1>
    <p>
      Posted on Sat 31 August 2024 in <a href="https://kashishchanana.github.io/category/scratch.html">Scratch</a>

    </p>
  </header>


  <div>
    <p>It's the Labor Day weekend, and I have found myself an amazing coding workshop by Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka, so here I am writing, understanding and building 'A Simple Tokenizer' from scratch. The figures in this blog post are taken from Sebastian Raschka's book "Build a Large Language Model From Scratch" until explicitly mentioned otherwise and the code is inspired from the aforementioned YouTube video. So let's get started, shall we?</p>
<p>Below is a pipeline of steps needed to build a large language model from scratch. We're currently at Stage 1, Step 1.</p>
<p><img alt="LLM-Pipeline" src="images/tokenizer/pipeline.png"></p>
<h3>Text Tokenization</h3>
<p>Text tokenization means breaking text into smaller units, such as individual words and punctuation characters. </p>
<p><img alt="Example Tokenized" src="images/tokenizer/example_tokenized.png"></p>
<p>To take an example text, let us assume the text we are building our LLM on is contained to <a href="https://en.wikisource.org/wiki/The_Verdict">The Verdict by Edith Wharton</a> which is a public domain short story.</p>
<div class="highlight"><pre><span></span><code><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;data/verdict.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample text&quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Raw text length:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of unique characters:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">text</span><span class="p">)))</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Sample text I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g
Raw text length: 20479
Number of unique characters: 62
</code></pre></div>

<p>A simple tokenizer can be built of simply using regular expressions. The regular expression will split the words by whitespaces, punctuations to yield smaller chunks of words.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">re</span>

<span class="c1"># Split the text into words and punctuation marks using regular expressions</span>
<span class="n">preprocessed_text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;([,.:;?!</span><span class="si">{}</span><span class="s2">()_=]|--|\s)&quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample preprocessed text&quot;</span><span class="p">,</span> <span class="n">preprocessed_text</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of tokens (words and punctuation marks):&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of unique tokens (words and punctuation marks):&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">)))</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Sample preprocessed text [&#39;I&#39;, &#39; &#39;, &#39;HAD&#39;, &#39; &#39;, &#39;always&#39;, &#39; &#39;, &#39;thought&#39;, &#39; &#39;, &#39;Jack&#39;, &#39; &#39;, &#39;Gisburn&#39;, &#39; &#39;, &#39;rather&#39;, &#39; &#39;, &#39;a&#39;, &#39; &#39;, &#39;cheap&#39;, &#39; &#39;, &#39;genius&#39;, &#39;--&#39;, &#39;though&#39;, &#39; &#39;, &#39;a&#39;, &#39; &#39;, &#39;good&#39;, &#39; &#39;, &#39;fellow&#39;, &#39; &#39;, &#39;enough&#39;, &#39;--&#39;, &#39;so&#39;, &#39; &#39;, &#39;it&#39;, &#39; &#39;, &#39;was&#39;, &#39; &#39;, &#39;no&#39;, &#39; &#39;, &#39;great&#39;, &#39; &#39;, &#39;surprise&#39;, &#39; &#39;, &#39;to&#39;, &#39; &#39;, &#39;me&#39;, &#39; &#39;, &#39;to&#39;, &#39; &#39;, &#39;hear&#39;, &#39; &#39;, &#39;that&#39;, &#39;,&#39;, &#39;&#39;, &#39; &#39;, &#39;in&#39;, &#39; &#39;, &#39;the&#39;, &#39; &#39;, &#39;height&#39;, &#39; &#39;, &#39;of&#39;, &#39; &#39;, &#39;his&#39;, &#39; &#39;, &#39;glory&#39;, &#39;,&#39;, &#39;&#39;, &#39; &#39;, &#39;he&#39;, &#39; &#39;, &#39;had&#39;, &#39; &#39;, &#39;dropped&#39;, &#39; &#39;, &#39;his&#39;, &#39; &#39;, &#39;painting&#39;, &#39;,&#39;, &#39;&#39;, &#39; &#39;, &#39;married&#39;, &#39; &#39;, &#39;a&#39;, &#39; &#39;, &#39;rich&#39;, &#39; &#39;, &#39;widow&#39;, &#39;,&#39;, &#39;&#39;, &#39; &#39;, &#39;and&#39;, &#39; &#39;, &#39;established&#39;, &#39; &#39;, &#39;himself&#39;, &#39; &#39;, &#39;in&#39;, &#39; &#39;, &#39;a&#39;, &#39; &#39;]
Number of tokens (words and punctuation marks): 8773
Number of unique tokens (words and punctuation marks): 1186
</code></pre></div>

<h3>Convert token to token IDs</h3>
<p>Now that we have tokens, it is vital to convert the text tokens into token IDs that we can process via embedding layers later. </p>
<p><img alt="Token Ids" src="images/tokenizer/tokenIds.png"></p>
<p>Consider this as a unique mapping of word to integer that represent the word. For this, we will build a vocabulary for all words in the dataset. Extract all unique tokens and assign unique token id. The vocabulary thus contains all unique words in the input text.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Build the vocabulary</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary size:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span> <span class="c1"># Vocabulary size: 1186</span>

<span class="c1"># Assign an index to each word in the vocabulary and create a mapping between words and indices, i.e, encode</span>
<span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>

<span class="c1"># Similarly, we can build an index to word dictionary to easily convert indices back to words i.e, decode</span>
<span class="n">index_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>

<span class="c1"># Example of encoding and decoding</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoding of &#39;the&#39;:&quot;</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">[</span><span class="s2">&quot;the&quot;</span><span class="p">])</span> <span class="c1"># Encoding of &#39;the&#39;: 1038</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoding of 846:&quot;</span><span class="p">,</span> <span class="n">index_to_word</span><span class="p">[</span><span class="mi">846</span><span class="p">])</span> <span class="c1"># Decoding of 846: pretty</span>
</code></pre></div>

<p>This can be tied together in a Simple Tokenizer class, that takes care of chunking the data using regular expressions and further exposes encoding and decoding functions.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">SimpleTokenizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;([,.:;?!</span><span class="si">{}</span><span class="s2">()_=]|--|\s)&quot;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">preprocessed_text</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SimpleTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&quot;It&#39;s the last he painted, you know, Mrs. Gisburn said with pardonable pride.&quot;&quot;&quot;</span>

<span class="n">encoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoded text:&quot;</span><span class="p">,</span> <span class="n">encoded_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoded text:&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_text</span><span class="p">))</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Encoded text: [21, 2, 1038, 2, 659, 2, 585, 2, 802, 66, 0, 2, 1180, 2, 653, 66, 0, 2, 122, 68, 0, 2, 93, 2, 905, 2, 1162, 2, 810, 2, 849, 68, 0]
Decoded text: &quot;It&#39;s the last he painted, you know, Mrs. Gisburn said with pardonable pride.
</code></pre></div>

<h3>BytePair Encoding</h3>
<p>The issue with our tokenizer is that it cannot handle unknown characters outside our vocabulary range. It is tied to the words already seen. GPT-2 used BytePair encoding (BPE) as its tokenizer. It allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words. For instance, if GPT-2's vocabulary doesn't have the word "unfamiliarword," it might tokenize it as ["unfam", "iliar", "word"] or some other subword breakdown, depending on its trained BPE merges. The original BPE tokenizer can be found here, <a href="https://github.com/openai/gpt-2/blob/master/src/encoder.py">BPE.</a></p>
<p>Suppose we have data aaabdaaabac which needs to be encoded (compressed). The byte pair aa occurs most often, so we will replace it with Z as Z does not occur in our data. So we now have ZabdZabac where Z = aa. The next common byte pair is ab so let’s replace it with Y. We now have ZYdZYac where Z = aa and Y = ab. The only byte pair left is ac which appears as just one so we will not encode it. We can use recursive byte pair encoding to encode ZY as X. Our data has now transformed into XdXac where X = ZY, Y = ab, and Z = aa. It cannot be further compressed as there are no byte pairs appearing more than once. We decompress the data by performing replacements in reverse order.</p>
<p>BPE tokenizer from OpenAI's open-source <a href="https://github.com/openai/tiktoken">tiktoken</a> library, Tiktoken, implements its core algorithms in Rust to improve computational performance.</p>
<h3>Data Sampling with Sliding Window</h3>
<p>After converting text into word tokens represented as token ID numbers, we need to create the data loading for LLMs. LLMs are trained to generate one word at a time, so we prepare the training data accordingly where the next word in a sequence represents the target to predict.</p>
<p><img alt="One Word Prediction" src="images/tokenizer/one-word-prediction.png"></p>
<p>To create the training set, a sliding window approach is used which adjusts the input position by +1 for the next pair of input-output sets. Input-output pairs thus look like-</p>
<div class="highlight"><pre><span></span><code>&lt;&#39;LLMs learn to&#39;, &#39;learn to predict&#39;&gt;
&lt;&#39;learn to predict&#39;, &#39;to predict one&#39;&gt;
&lt;&#39;to predict one&#39;, &#39;predict one word&#39;&gt; 
</code></pre></div>

<p>... and so on. Essentially the target's last word is the predicted word. </p>
<p>You might notice that we do have several repeated phrases in our training input set now, to avoid this, we use stride as a way to avoid overlaps. In practice it's best to set the stride equal to the context length so that we don't have overlaps between the inputs (the targets are still shifted by +1 always).</p>
<p><img alt="Stride in Prediction" src="images/tokenizer/stride.png"></p>
<h3>Creating a DataLoader</h3>
<p>From above, we can create a PyTorch dataset with input-target pairs.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>


<span class="k">class</span> <span class="nc">GPTDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">allowed_special</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;&lt;|endoftext|&gt;&quot;</span><span class="p">})</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">-</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
            <span class="c1"># Append the tokens from i to i + context_length to the input list</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">context_length</span><span class="p">]))</span>
            <span class="c1"># Append the tokens from i + 1 to i + context_length + 1 to the target list</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="n">context_length</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>


<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">GPTDataset</span><span class="p">(</span><span class="n">verdict_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">context_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span> <span class="c1"># In practice, context length is 1024 for gpt2</span>

<span class="n">dataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>(tensor([   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,
           257,  7026, 15632,   438,  2016,   257]),
 tensor([  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,
          7026, 15632,   438,  2016,   257,   922]))
</code></pre></div>

<p>As can be seen from above, the last token in input is 257, which forms the second last token for target and the next predicted token as 922. If we see the next input-output target, we can see that there is no overlapping of tokens in the inputs.</p>
<div class="highlight"><pre><span></span><code><span class="n">dataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>(tensor([ 922, 5891, 1576,  438,  568,  340,  373,  645, 1049, 5975,  284,  502,
          284, 3285,  326,   11]),
 tensor([5891, 1576,  438,  568,  340,  373,  645, 1049, 5975,  284,  502,  284,
         3285,  326,   11,  287]))
</code></pre></div>

<p>We can further form a DataLoader from this Dataset as below -</p>
<div class="highlight"><pre><span></span><code><span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<p>And voila, you've created a custom dataloader for your dataset.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://kashishchanana.github.io/tag/llms-from-scratch.html">LLMs From Scratch</a>
      <a href="https://kashishchanana.github.io/tag/tokenizer.html">Tokenizer</a>
    </p>
  </div>






</article>

<footer>
<p>&copy; 2024 Kashish Chanana</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Hitchhiker's Guide To AI ",
  "url" : "https://kashishchanana.github.io",
  "image": "images/test/ai.png",
  "description": ""
}
</script>
</body>
</html>