
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="https://kashishchanana.github.io/theme/pygments/friendly.min.css">



  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/solid.css">












 

<meta name="author" content="Kashish Chanana" />
<meta name="description" content="RAG or Naive RAG is the concept of providing an LLM with additional context from an external knowledge source for knowledge injection and to reduce hallucinations. Although the concept of Retrieval-Augmented Generation (RAG) was already published in a paper by Lewis et al. in 2020 , it has gained a lot …" />
<meta name="keywords" content="RAG, LLMs, Vector-Databases">


  <meta property="og:site_name" content="Hitchhiker's Guide To AI"/>
  <meta property="og:title" content="Building Retrieval Augmented Generation Systems"/>
  <meta property="og:description" content="RAG or Naive RAG is the concept of providing an LLM with additional context from an external knowledge source for knowledge injection and to reduce hallucinations. Although the concept of Retrieval-Augmented Generation (RAG) was already published in a paper by Lewis et al. in 2020 , it has gained a lot …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://kashishchanana.github.io/building-retrieval-augmented-generation-systems.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2024-04-28 00:00:00-07:00"/>
  <meta property="article:modified_time" content="2024-04-28 00:00:00-07:00"/>
  <meta property="article:author" content="https://kashishchanana.github.io/author/kashish-chanana.html">
  <meta property="article:section" content="LLMs"/>
  <meta property="article:tag" content="RAG"/>
  <meta property="article:tag" content="LLMs"/>
  <meta property="article:tag" content="Vector-Databases"/>
  <meta property="og:image" content="images/test/ai.png">

  <title>Hitchhiker's Guide To AI &ndash; Building Retrieval Augmented Generation Systems</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="https://kashishchanana.github.io/">
      <img src="images/test/ai.png" alt="" title="">
    </a>

    <h1>
      <a href="https://kashishchanana.github.io/"></a>
    </h1>

    <p>Honestly started as a note-taking exercise!</p>



    <ul class="social">
      <li>
        <a class="sc-github"
           href="https://github.com/KashishChanana"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-linkedin"
           href="https://www.linkedin.com/in/kashishchanana/"
           target="_blank">
          <i class="fa-brands fa-linkedin"></i>
        </a>
      </li>
      <li>
        <a class="sc-twitter"
           href="https://twitter.com/chankashish"
           target="_blank">
          <i class="fa-brands fa-twitter"></i>
        </a>
      </li>
      <li>
        <a class="sc-envelope"
rel="me"           href="mailto:chananakashish1998@gmail.com"
           target="_blank">
          <i class="fa-solid fa-envelope"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>

<nav>
  <a href="https://kashishchanana.github.io/">Home</a>

  <a href="/archives">Archives</a>
  <a href="/categories">Categories</a>
  <a href="/tags">Tags</a>


</nav>

<article class="single">
  <header>
      
    <h1 id="building-retrieval-augmented-generation-systems">Building Retrieval Augmented Generation Systems</h1>
    <p>
      Posted on Sun 28 April 2024 in <a href="https://kashishchanana.github.io/category/llms.html">LLMs</a>

    </p>
  </header>


  <div>
    <!-- Status: Draft -->

<p>RAG or Naive RAG is the concept of providing an LLM with additional context from an external knowledge source for knowledge injection and to reduce hallucinations. Although the concept of Retrieval-Augmented Generation (RAG) was already published in a paper by <a href="https://arxiv.org/abs/2005.11401"> Lewis et al. in 2020 </a>, it has gained a lot of interest since the release of ChatGPT.</p>
<p>It consists of two stages. First, an external knowledge source is prepared in the ingestion stage.
In the inference stage, this knowledge source is used to provide additional context to a user query. The retrieved context and user query are then used to augment a prompt template, which is used to generate an answer to the original user query.</p>
<p>This blog discusses a Naive RAG Pipeline, as provides pronounced code to perform QA on a YouTube Video. For the purpose of this blog, the YouTube video taken into account is <a href = "https://www.youtube.com/watch?v=zjkBMFhNj_g"> [1hr Talk] Intro to Large Language Models by Andrej Karpathy </a></p>
<p><img alt="RAG Pipeline" src="images/rag/RAG.png"></p>
<p>There are 5 steps that will be discussed. The first three belong to Ingestion stage and the last two belong to Inference stage.</p>
<ol>
<li>Data Loading</li>
<li>Data Splitting</li>
<li>Vectore Storage</li>
<li>Semantic Retrieval</li>
<li>Output</li>
</ol>
<h3>Preset</h3>
<div class="highlight"><pre><span></span><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">langchain</span> <span class="n">openai</span> <span class="n">chromadb</span> <span class="n">pytube</span>

<span class="c1"># Using Open AI’s APIs to make direct API calls to LLM.</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">openai</span>

<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span> <span class="c1"># read local .env file</span>
<span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;OPENAI_API_KEY&#39;</span><span class="p">]</span>
</code></pre></div>

<hr>
<h3>Document Loading</h3>
<p>In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution. This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc). Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. There are several document loaders that LangChain provides such as TextLoader, PyPDFLoader</p>
<p>In our case, this information will come via the <code>YouTube Loader</code> that LangChain offers.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">YoutubeLoader</span>
<span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="o">--</span><span class="n">quiet</span>  <span class="n">youtube</span><span class="o">-</span><span class="n">transcript</span><span class="o">-</span><span class="n">api</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">YoutubeLoader</span><span class="o">.</span><span class="n">from_youtube_url</span><span class="p">(</span><span class="n">youtube_url</span><span class="o">=</span><span class="s2">&quot;https://www.youtube.com/watch?v=zjkBMFhNj_g&quot;</span><span class="p">,</span> 
                                        <span class="n">add_video_info</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">transcript</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
</code></pre></div>

<hr>
<h3>Document Splitting</h3>
<p>Once you've loaded documents, you'll want to transform them to better suit your application. You may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.</p>
<p><img alt="Text Splitter" src="images/chunk.png"></p>
<p>Recursive Splitter - Recursively splits text. Splitting text recursively serves the purpose of trying to keep related pieces of text next to each other. RecursiveCharacterTextSplitter is recommended for generic text.</p>
<p>Token Splitter - Splits text on tokens. There exist a few different ways to measure tokens.This can be useful because LLMs often have context windows designated in tokens. Tokens are often ~4 characters.</p>
<p>Context aware splitting - Chunking aims to keep text with common context together. A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting. Use MarkdownHeaderTextSplitter to preserve header metadata in our chunks. When reading information from Notion, it can be beneficial to use MarkdownHeaderTextSplitter.</p>
<p>In our case, we'll go with RecursiveCharacterTextSplitter with a <code>chunk_size</code> of 500 characters and a <code>chunk_overlap</code> of 25 chars.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
                                               <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">transcript</span><span class="p">)</span>
</code></pre></div>

<hr>
<h3>Vector Stores and Embeddings</h3>
<p>Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and perform semantic search where we look for pieces of text that are most similar in the vector space.</p>
<p><img alt="Embedding Similarity" src="images/embedding_similarity.png"></p>
<p>One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you.
Let's start by creating the embeddings of the docs. We'll employ <code>Chroma DB</code> as our vector store.</p>
<p><img alt="Create Embeddings" src="images/create_embeds.png"></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings.openai</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>


<span class="n">persist_directory</span> <span class="o">=</span> <span class="s1">&#39;docs/chroma/&#39;</span>


<span class="n">vectordb</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="n">docs</span><span class="p">,</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">embedding</span><span class="p">,</span>
    <span class="n">persist_directory</span><span class="o">=</span><span class="n">persist_directory</span>
<span class="p">)</span>

<span class="n">We</span> <span class="n">can</span> <span class="n">now</span> <span class="n">perform</span> <span class="n">a</span> <span class="n">search</span> <span class="n">on</span> <span class="n">this</span> <span class="n">vector</span> <span class="n">database</span> <span class="n">using</span> <span class="n">a</span> <span class="n">query</span><span class="o">.</span> <span class="n">We</span> <span class="n">can</span> <span class="n">either</span> <span class="n">do</span> <span class="n">a</span> <span class="err">`</span><span class="n">similarity</span> <span class="n">search</span><span class="err">`</span> <span class="ow">or</span> <span class="n">do</span> <span class="n">a</span> <span class="err">`</span><span class="n">similarity</span> <span class="n">search</span> <span class="n">by</span> <span class="n">vector</span><span class="err">`</span><span class="o">.</span> <span class="n">It</span> <span class="ow">is</span> <span class="n">also</span> <span class="n">possible</span> <span class="n">to</span> <span class="n">do</span> <span class="n">a</span> <span class="n">search</span> <span class="k">for</span> <span class="n">documents</span> <span class="n">similar</span> <span class="n">to</span> <span class="n">a</span> <span class="n">given</span> <span class="n">embedding</span> <span class="n">vector</span> <span class="n">using</span> <span class="n">similarity_search_by_vector</span> <span class="n">which</span> <span class="n">accepts</span> <span class="n">an</span> <span class="n">embedding</span> <span class="n">vector</span> <span class="k">as</span> <span class="n">a</span> <span class="n">parameter</span> <span class="n">instead</span> <span class="n">of</span> <span class="n">a</span> <span class="n">string</span><span class="o">.</span> <span class="n">Here</span> <span class="n">the</span> <span class="n">parameter</span> <span class="err">`</span><span class="n">k</span><span class="err">`</span> <span class="n">represents</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">docs</span> <span class="n">that</span> <span class="n">will</span> <span class="n">be</span> <span class="n">retrieved</span> <span class="n">based</span> <span class="n">on</span> <span class="n">the</span> <span class="n">query</span><span class="o">.</span>
</code></pre></div>

<p><img alt="Retrieve Embeddings" src="images/retrieve_embeds.png"></p>
<p>Similarity Search</p>
<div class="highlight"><pre><span></span><code>question = &quot;What was said about LLM OS?&quot;
retrieved_docs = vectordb.similarity_search(
    query=question,
    k=3)
</code></pre></div>

<p>Similarity search by vector</p>
<div class="highlight"><pre><span></span><code>embedding_vector = OpenAIEmbeddings().embed_query(question)
retrieved_docs = vectordb.similarity_search_by_vector(
    embedding_vector,
    k=3)
</code></pre></div>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://kashishchanana.github.io/tag/rag.html">RAG</a>
      <a href="https://kashishchanana.github.io/tag/llms.html">LLMs</a>
      <a href="https://kashishchanana.github.io/tag/vector-databases.html">Vector-Databases</a>
    </p>
  </div>






</article>

<footer>
<p>&copy; 2024 Kashish Chanana</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Hitchhiker's Guide To AI ",
  "url" : "https://kashishchanana.github.io",
  "image": "images/test/ai.png",
  "description": ""
}
</script>
</body>
</html>