
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
  <link rel="icon" type="image/x-icon" href="/images/favicon.ico">

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="https://kashishchanana.github.io/theme/pygments/friendly.min.css">



  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/solid.css">












 

<meta name="author" content="Kashish Chanana" />
<meta name="description" content="This blog post is a Part II of building an LLM from scratch. This is yet again inspired by a coding workshop - Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka. As in the below diagram of contructing an LLM ground up, let&#39;s assume that we&#39;ve …" />
<meta name="keywords" content="LLMs From Scratch, Architecture">


  <meta property="og:site_name" content="Hitchhiker's Guide To AI"/>
  <meta property="og:title" content="Coding an LLM Architecture"/>
  <meta property="og:description" content="This blog post is a Part II of building an LLM from scratch. This is yet again inspired by a coding workshop - Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka. As in the below diagram of contructing an LLM ground up, let&#39;s assume that we&#39;ve …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://kashishchanana.github.io/coding-an-llm-architecture.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2024-09-01 00:00:00-07:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="https://kashishchanana.github.io/author/kashish-chanana.html">
  <meta property="article:section" content="LLMs"/>
  <meta property="article:tag" content="LLMs From Scratch"/>
  <meta property="article:tag" content="Architecture"/>
  <meta property="og:image" content="images/test/ai.png">

  <title>Hitchhiker's Guide To AI &ndash; Coding an LLM Architecture</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="https://kashishchanana.github.io/">
      <img src="images/test/ai.png" alt="" title="">
    </a>

    <h1>
      <a href="https://kashishchanana.github.io/"></a>
    </h1>

    <p>Honestly started as a note-taking exercise!</p>



    <ul class="social">
      <li>
        <a class="sc-github"
           href="https://github.com/KashishChanana"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-linkedin"
           href="https://www.linkedin.com/in/kashishchanana/"
           target="_blank">
          <i class="fa-brands fa-linkedin"></i>
        </a>
      </li>
      <li>
        <a class="sc-twitter"
           href="https://twitter.com/chankashish"
           target="_blank">
          <i class="fa-brands fa-twitter"></i>
        </a>
      </li>
      <li>
        <a class="sc-envelope"
rel="me"           href="mailto:chananakashish1998@gmail.com"
           target="_blank">
          <i class="fa-solid fa-envelope"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>

<nav>
  <a href="https://kashishchanana.github.io/">Home</a>

  <a href="/archives">Archives</a>
  <a href="/categories">Categories</a>
  <a href="/tags">Tags</a>


</nav>

<article class="single">
  <header>
      
    <h1 id="coding-an-llm-architecture">Coding an LLM Architecture</h1>
    <p>
      Posted on Sun 01 September 2024 in <a href="https://kashishchanana.github.io/category/llms.html">LLMs</a>

    </p>
  </header>


  <div>
    <p>This blog post is a Part II of building an LLM from scratch. This is yet again inspired by a coding workshop - Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka. As in the below diagram of contructing an LLM ground up, let's assume that we've a black box 'Attention Mechanism' ready with us, and we are jumping straight ahead to Part 3, LLM Architecture. I'd revisit Part 2 Attention Mechanism in a different blog post. This way of learning helps me, as I get to understand the high level architecture before diving deep into any of the phases! We will begin with a top-down view of the model architecture in the next section before covering the individual components in more detail.</p>
<p><img alt="LLM-Pipeline" src="images/llm-architecture/pipeline.png"></p>
<h3 id="coding-a-gpt-like-llm">Coding a GPT-like LLM</h3>
<p>LLMs, such as GPT (which stands for Generative Pretrained Transformer), are large deep
neural network architectures designed to generate new text one word (or token) at a time. Below is a mental model of a GPT model. The tokenized text moves into the embedding layers, and further to it one or more transformer blocks containing the masked multi-head attention module. It is then followed by a set of output layers. The goal remains to generate one word at a time.</p>
<p><img alt="GPT Model" src="images/llm-architecture/raw-llm-archi.png"></p>
<p>The sizing of models, GPT-2 'small' vs GPT-2 'large' is due to the number of transformer blocks and multi head attention heads in the construction. GPT-2 small has 12 tranformer blocks with 12 attention heads each whereas GPT 2 large contains 36 transformer blocks with 20 attention heads each. Compared to conventional deep learning models, LLMs are larger, mainly due to their vast number of parameters, not the amount of code.</p>
<p><img alt="GPT-2-Architecture" src="images/llm-architecture/gpt2-arci.png"></p>
<p>The construction of the GPT model remains fairly consistent across the various offerings of large language models (GPT, Gemma, Phi, Mistral, Llama etc.) as they are all essentially based on the same concepts. They do have a few additions/subtractions. </p>
<p><img alt="GPT-Llama-Resemblence" src="images/llm-architecture/gpt2vsllama.png"></p>
<p>The DummyGPTModel class in below code defines a simplified version of a GPT-like model usingPyTorch's neural network module (nn.Module). The model architecture in the DummyGPTModel class consists of token and positional embeddings, dropout, a series of transformer blocks (DummyTransformerBlock), a final layer normalization (DummyLayerNorm), and a linear output layer (out_head). The configuration of the small GPT-2 model via the following Python dictionary.</p>
<div class="highlight"><pre><span></span><code><span class="n">GPT_CONFIG_124M</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">50257</span><span class="p">,</span>    <span class="c1"># Vocabulary size refers to a vocabulary of 50,257 words, as used by the BPE tokenizer </span>
    <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span> <span class="c1"># Context length denotes the maximum number of input tokens the model can handle</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>         <span class="c1"># Embedding dimension represents the embedding size,</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>          <span class="c1"># Number of attention heads in the multi-head attention mechanism</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>         <span class="c1"># Number of layers, i.e, the number of transformer blocks in the model</span>
    <span class="s2">&quot;drop_rate&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>       <span class="c1"># Dropout rate, indicates the intensity of the dropout mechanism </span>
    <span class="s2">&quot;qkv_bias&quot;</span><span class="p">:</span> <span class="kc">False</span>       <span class="c1"># Query-Key-Value bias, determines whether to include a bias vector in the Linear</span>
                            <span class="c1"># layers of the multi-head attention for query, key, and value computations</span>
<span class="p">}</span>
</code></pre></div>

<p>The forward method describes the data flow through the model: it computes token and positional embeddings for the input indices, applies dropout, processes the data through the transformer blocks, applies normalization, and finally produces logits with the linear output layer.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DummyGPTModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> 
                                      <span class="n">embedding_dim</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span> 
                                         <span class="n">embedding_dim</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">DummyTransformerBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">DummyLayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">])</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_idx</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">in_idx</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">token_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_emb</span><span class="p">(</span><span class="n">in_idx</span><span class="p">)</span>
        <span class="n">position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_emb</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">in_idx</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">token_embeddings</span> <span class="o">+</span> <span class="n">position_embeddings</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>
</code></pre></div>

<p><img alt="GPT-prediction-example" src="images/llm-architecture/gpt-pred-example.png"></p>
<p>Assuming that the DummyTransformerBlock and DummyLayerNorm are implemented. To generate the next set of words, we tokenize a batch consisting of two text inputs for the GPT model using the tiktoken tokenizer. </p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">tiktoken</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">txt1</span> <span class="o">=</span> <span class="s2">&quot;Every effort moves you&quot;</span>
<span class="n">txt2</span> <span class="o">=</span> <span class="s2">&quot;Every day holds a&quot;</span>

<span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">txt1</span><span class="p">)))</span>
<span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">txt2</span><span class="p">)))</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</code></pre></div>

<p>The resulting token IDs for the two texts are as follows:</p>
<div class="highlight"><pre><span></span><code>tensor([[6109, 3626, 6100,  345],
        [6109, 1110, 6622,  257]])
</code></pre></div>

<p>Next, we initialize a new 124 million parameter DummyGPTModel instance and feed it the tokenized batch.</p>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">DummyGPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input shape:&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output shape:&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div>

<p>The model outputs, which are commonly referred to as logits, are as follows -</p>
<div class="highlight"><pre><span></span><code>Input shape: torch.Size([2, 4])
Output shape: torch.Size([2, 4, 50257])
tensor([[[-0.0061,  0.2018, -0.1894,  ...,  0.6021,  0.0023, -0.6446],
         [ 0.5441, -0.0973, -0.6294,  ..., -0.1285,  0.4564,  0.2919],
         [ 1.0331, -0.0616,  0.0528,  ...,  0.6333, -0.3860, -0.3187],
         [-0.6778,  0.0553, -0.3303,  ...,  1.1527,  0.1931, -0.7693]],

        [[ 0.1539,  0.0028,  0.0655,  ..., -0.1306,  0.5186, -0.1966],
         [ 0.0415, -0.2014,  0.1478,  ...,  0.6864, -0.2254,  0.4057],
         [ 1.0918,  0.6361, -0.0494,  ...,  0.6526,  0.5259, -0.1022],
         [ 0.5160,  0.2702,  0.6313,  ...,  0.7390,  0.0222,  0.0687]]],
       grad_fn=&lt;ViewBackward0&gt;)
</code></pre></div>

<p>The output tensor has two rows corresponding to the two text samples. Each text sample consists of 4 tokens; each token is a 50,257-dimensional vector, which matches the size of the tokenizer's vocabulary. The embedding has 50,257 dimensions because each of these dimensions refers to a unique token in the vocabulary. The GPTModel implementation outputs
tensors with shape <code>[batch_size, num_token, vocab_size]</code>. </p>
<p>Below is a step-by-step process by which a GPT model generates text given an input context, such as "Hello, I am," on a big-picture level. With each iteration, the input context grows, allowing the model to generate coherent and contextually appropriate text. By the 6th iteration, the model has constructed a complete sentence: "Hello, I am a model ready to help."</p>
<p><img alt="Next Token Prediction" src="images/llm-architecture/next-token-prediction.png"></p>
<p>Now, the question is, how does a GPT model go from these output tensors to the generated text?
In each step, the model outputs a matrix with vectors representing potential next tokens. The vector corresponding to the next token is extracted and converted into a probability distribution via the softmax function. Within the vector containing the resulting probability scores, the index of the highest value is located, which translates to the token ID. This token ID is then decoded back into text, producing the next token in the sequence. Finally, this token is appended to the previous inputs, forming a new input sequence for the subsequent iteration. In practice, we repeat this process over many iterations until we reach a user-specified number of generated tokens.</p>
<p><img alt="Generating next token" src="images/llm-architecture/softmax-logits.png"></p>
<p>In the below code, the generate_text_simple function, we use a softmax function to convert the logits into a probability distribution from which we identify the position with the highest value via torch.argmax. The softmax function is <code>monotonic</code>, meaning it preserves the order of its inputs when transformed into outputs. So, in practice, the softmax step is
redundant since the position with the highest score in the softmax output tensor is the same position in the logit tensor. In other words, we could apply the <code>torch.argmax</code> function to the logits tensor directly and get identical results. However, the conversion to illustrate the full process of transforming logits to probabilities, which can add additional
intuition, such as that the model generates the most likely next token, which is known as <code>greedy decoding</code>.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">generate_text_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">context_size</span><span class="p">):</span>
    <span class="c1"># idx is (batch, n_tokens) array of indices in the current context</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>

        <span class="c1"># Crop current context if it exceeds the supported context size</span>
        <span class="n">context_tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:,</span> <span class="o">-</span><span class="n">context_size</span><span class="p">:]</span>

        <span class="c1"># Get the predictions</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">context_tokens</span><span class="p">)</span>


        <span class="c1"># Focus only on the last time step</span>
        <span class="c1"># (batch, n_tokens, vocab_size) becomes (batch, vocab_size)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  

        <span class="c1"># Apply softmax to get probabilities</span>
        <span class="n">probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, vocab_size)</span>

        <span class="c1"># Get the idx of the vocab entry with the highest probability value</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (batch, 1)</span>

        <span class="c1"># Append sampled index to the running sequence</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">tokens</span><span class="p">,</span> <span class="n">next_token</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, n_tokens+1)</span>

    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div>

<p>Now, to generate the next words in the sentence <code>Hey, how are you doing today?</code>, we will pass this text tokens to our generate_text_simple function and use the decode method of the tokenizer to get human readable word.</p>
<div class="highlight"><pre><span></span><code><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1"># Set model to evaluation mode</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Hey, how are you doing today?&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">generated_tokens</span> <span class="o">=</span> <span class="n">generate_text_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generated tokens:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">generated_tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generated text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Generated tokens:
 tensor([[10814,    11,   703,   389,   345,  1804,  1909,    30, 32059, 47952,
         39382, 50160, 21458, 26766, 37602, 16006, 25952, 33724]])
Generated text:
 Hey, how are you doing today?767 testifying detects collaborator BooSilver Rudy cycles SSL burglary
</code></pre></div>

<p>As can be seen from above, a rather gibberish sentence has been predicted to follow our given input. The reason why the model is unable to produce coherent text is that we haven't trained it yet. So far, we just implemented the GPT architecture and initialized a GPT model instance with initial random weights.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://kashishchanana.github.io/tag/llms-from-scratch.html">LLMs From Scratch</a>
      <a href="https://kashishchanana.github.io/tag/architecture.html">Architecture</a>
    </p>
  </div>






</article>

<footer>
<p>&copy; 2024 Kashish Chanana</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Hitchhiker's Guide To AI ",
  "url" : "https://kashishchanana.github.io",
  "image": "images/test/ai.png",
  "description": ""
}
</script>
</body>
</html>