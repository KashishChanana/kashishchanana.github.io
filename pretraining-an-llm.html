
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
  <link rel="icon" type="image/x-icon" href="/images/favicon.ico">

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="https://kashishchanana.github.io/theme/pygments/friendly.min.css">



  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://kashishchanana.github.io/theme/font-awesome/css/solid.css">












 

<meta name="author" content="Kashish Chanana" />
<meta name="description" content="This blog post is a Part III of building an LLM from scratch. Yet another post based out of a coding workshop - Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka. This blog is based out of training the LLM on the data that was created …" />
<meta name="keywords" content="LLMs From Scratch, PreTraining">


  <meta property="og:site_name" content="Hitchhiker's Guide To AI"/>
  <meta property="og:title" content="Pretraining an LLM"/>
  <meta property="og:description" content="This blog post is a Part III of building an LLM from scratch. Yet another post based out of a coding workshop - Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka. This blog is based out of training the LLM on the data that was created …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://kashishchanana.github.io/pretraining-an-llm.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2024-09-02 00:00:00-07:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="https://kashishchanana.github.io/author/kashish-chanana.html">
  <meta property="article:section" content="Scratch"/>
  <meta property="article:tag" content="LLMs From Scratch"/>
  <meta property="article:tag" content="PreTraining"/>
  <meta property="og:image" content="images/test/ai.png">

  <title>Hitchhiker's Guide To AI &ndash; Pretraining an LLM</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="https://kashishchanana.github.io/">
      <img src="images/test/ai.png" alt="" title="">
    </a>

    <h1>
      <a href="https://kashishchanana.github.io/"></a>
    </h1>

    <p>Honestly started as a note-taking exercise!</p>



    <ul class="social">
      <li>
        <a class="sc-github"
           href="https://github.com/KashishChanana"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-linkedin"
           href="https://www.linkedin.com/in/kashishchanana/"
           target="_blank">
          <i class="fa-brands fa-linkedin"></i>
        </a>
      </li>
      <li>
        <a class="sc-twitter"
           href="https://twitter.com/chankashish"
           target="_blank">
          <i class="fa-brands fa-twitter"></i>
        </a>
      </li>
      <li>
        <a class="sc-envelope"
rel="me"           href="mailto:chananakashish1998@gmail.com"
           target="_blank">
          <i class="fa-solid fa-envelope"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>

<nav>
  <a href="https://kashishchanana.github.io/">Home</a>

  <a href="/archives">Archives</a>
  <a href="/categories">Categories</a>
  <a href="/tags">Tags</a>


</nav>

<article class="single">
  <header>
      
    <h1 id="pretraining-an-llm">Pretraining an LLM</h1>
    <p>
      Posted on Mon 02 September 2024 in <a href="https://kashishchanana.github.io/category/scratch.html">Scratch</a>

    </p>
  </header>


  <div>
    <p>This blog post is a Part III of building an LLM from scratch. Yet another post based out of a coding workshop - Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka. This blog is based out of training the LLM on the data that was created in Part I - A Simple Tokenizer. Since the dataset that is being used is of relatively small size (in fact, only one short story), the training finishes relatively fast (minutes instead of weeks). To put in context, the short story has 5145 tokens while Llama 2 7B was trained on 2 trillion tokens that required 184,320 GPU hours on A100 GPUs.</p>
<p><img alt="LLM-Pipeline" src="images/pretraining/pipeline.png"></p>
<div class="highlight"><pre><span></span><code><span class="n">train_test_split</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">split_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">verdict_text</span><span class="p">)</span> <span class="o">*</span> <span class="n">train_test_split</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">GPTDataset</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">verdict_text</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">],</span> 
                           <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
                           <span class="n">context_length</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;context_length&quot;</span><span class="p">),</span>
                           <span class="n">stride</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;context_length&quot;</span><span class="p">))</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">GPTDataset</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">verdict_text</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:],</span>
                        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
                        <span class="n">context_length</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;context_length&quot;</span><span class="p">),</span>
                        <span class="n">stride</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;context_length&quot;</span><span class="p">))</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://kashishchanana.github.io/tag/llms-from-scratch.html">LLMs From Scratch</a>
      <a href="https://kashishchanana.github.io/tag/pretraining.html">PreTraining</a>
    </p>
  </div>






</article>

<footer>
<p>&copy; 2024 Kashish Chanana</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Hitchhiker's Guide To AI ",
  "url" : "https://kashishchanana.github.io",
  "image": "images/test/ai.png",
  "description": ""
}
</script>
</body>
</html>